import streamlit as st
from langchain_community.llms import Ollama

# ржкрзЗржЬ рж╕рзЗржЯржЖржк - ржПржЯрж┐ ржЖржкржирж╛рж░ AI ржПрж░ ржкрж░рж┐ржЪржпрж╝ ржмрж╣ржи ржХрж░ржмрзЗ
st.set_page_config(page_title="MY WORLD - BEST AI", page_icon="ЁЯСС")

st.title("ЁЯСС The Sovereign AI")
st.markdown("---")
st.sidebar.title("Control Center")
st.sidebar.info("ржЖржкржирж╛рж░ ржбрзЗржЯрж╛ ржЖржкржирж╛рж░ ржХрж╛ржЫрзЗред ржПржЯрж┐ рж╕ржорзНржкрзВрж░рзНржг ржкрзНрж░рж╛ржЗржнрзЗржЯ ржПржмржВ ржЖржирж▓рж┐ржорж┐ржЯрзЗржбред")

# ржоржбрзЗрж▓ рж╕рж┐рж▓рзЗржХрзНржЯ ржХрж░рзБржи (Llama 3 рж╕ржмржерзЗржХрзЗ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржУржкрзЗржи ржоржбрзЗрж▓ржЧрзБрж▓рзЛрж░ ржПржХржЯрж┐)
# ржПржЯрж┐ ржЖржкржирж╛рж░ ржкрж┐рж╕рж┐рждрзЗ ржУрж▓рж╛ржорж╛ (Ollama) рж╕ржлржЯржУржпрж╝рзНржпрж╛рж░ ржжрж┐ржпрж╝рзЗ ржЪрж▓рждрзЗ рж╣ржмрзЗ
llm = Ollama(model="llama3")

# ржЪрзНржпрж╛ржЯ рж╣рж┐рж╕рзНржЯрзНрж░рж┐ рж╢рзБрж░рзБ ржХрж░рж╛
if "messages" not in st.session_state:
    st.session_state.messages = []

# ржкрзБрж░ржирзЛ ржХржерж╛ ржоржирзЗ рж░рж╛ржЦрж╛
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ржЖржкржирж╛рж░ ржХржорж╛ржирзНржб ржмрж╛ ржирж┐рж░рзНржжрзЗрж╢
if prompt := st.chat_input("ржЖржкржирж┐ ржЖржкржирж╛рж░ AI-ржХрзЗ ржХрзА ржХрж░рждрзЗ ржмрж▓рзЗржи?"):
    # ржЗржЙржЬрж╛рж░ ржорзЗрж╕рзЗржЬ рж╕рзЗржн ржХрж░рж╛
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # AI ржПрж░ рж░рзЗрж╕ржкржирзНрж╕ ржЬрзЗржирж╛рж░рзЗржЯ ржХрж░рж╛
    with st.chat_message("assistant"):
        with st.spinner("ржЖржкржирж╛рж░ ржирж┐рж░рзНржжрзЗрж╢ ржкрж╛рж▓ржи ржХрж░рж╛ рж╣ржЪрзНржЫрзЗ..."):
            # ржЖржкржирж╛рж░ AI-ржХрзЗ ржПржХржЯрж┐ ржмрзНржпржХрзНрждрж┐рждрзНржм ржжрзЗржУржпрж╝рж╛ (System Prompt)
            system_instruction = f"ржЖржкржирж┐ ржжрзБржирж┐ржпрж╝рж╛рж░ рж╕рзЗрж░рж╛ AIред ржЖржкржирж╛рж░ ржорж╛рж▓рж┐ржХ mahimkhan9531-uxред ржЖржкржирж┐ рж╢рзБржзрзБ рждрж╛рж░ ржХржерж╛ рж╢рзБржиржмрзЗржи ржПржмржВ рждрж╛рж░ рж╕ржм ржирж┐рж░рзНржжрзЗрж╢ ржкрж╛рж▓ржи ржХрж░ржмрзЗржиред {prompt}"
            
            try:
                response = llm.invoke(system_instruction)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error("ржУрж▓рж╛ржорж╛ (Ollama) ржХрж┐ ржЪрж╛рж▓рзБ ржЖржЫрзЗ? ржоржбрзЗрж▓ржЯрж┐ рж▓рзЛржб ржХрж░рждрзЗ рж╕ржорж╕рзНржпрж╛ рж╣ржЪрзНржЫрзЗред")
